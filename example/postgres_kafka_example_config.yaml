# Input source configuration
input:
  # Type of input connector. Currently supported: [postgres]
  connector: "postgres"

  # Configuration for PostgreSQL input. Used only if connector is set to "postgres".
  #
  # The "From To" application will:
  # - Create a table named "from_to_event" and necessary triggers for each table listed in input.postgresConfig.tables
  # - Use these triggers and the event table to enable change polling
  #
  # Notes:
  # - You can make the "from_to_event" table unlogged for performance
  # - You can delete old events from this table at any time
  # - You can review the exact SQL queries used here:
  #   https://github.com/gustapinto/from-to/blob/main/internal/connectors/postgres/queries.go
  postgresConfig:
    # PostgreSQL DSN (connection string). The user must have permissions to create tables, triggers, and functions.
    dsn: "postgres://from-to-user:from-to-passw@localhost:5432/from-to-db?sslmode=disable"

    # The maximum time that any query should take to complete (default: 30)
    timeoutSeconds: 5

    # How often to poll for new changes, in seconds (default: 30)
    pollSeconds: 5

    # Maximum number of records to process per polling cycle (default: 50)
    pollLimit: 10

    # List of table names to monitor for changes
    tables:
      - "sales"

# Output destination configuration
outputs:
  # Define an output target, referenced by name in the channels section
  salesKafkaOutput:
    # Type of output connector. Currently supported: [kafka]
    connector: "kafka"

    # Kafka-specific configuration. Used when connector is set to "kafka"
    kafkaConfig:
      # Kafka bootstrap server addresses
      bootstrapServers:
        - "localhost:9094"

      # List of Kafka topics to publish to
      topics:
        - name: "publicSales"        # Name of the Kafka topic
          partitions: 3              # Number of partitions (optional, default: 3)
          replicationFactor: 1      # Replication factor (optional, default: 1)

# Data transformation (mapper) configuration
mappers:
  # Define a mapper, referenced by name in the channels section
  salesMapper:
    # Type of mapper. Currently supported: [lua]
    type: "lua"

    # Lua-specific configuration. Used when type is "lua"
    luaConfig:
      # Path to the Lua script file
      filePath: "./example/mappers.lua"

      # Function name inside the Lua file that will handle the transformation
      function: "map_sales_event_with_http"

# Channel definitions â€” this is where you wire together input, mapper, and output
channels:
  # Define a channel (pipeline) by name
  salesChannel:
    # Source table to read changes from (must match one of input.postgresConfig.tables)
    from: "sales"

    # Destination name (e.g., Kafka topic)
    to: "publicSales"

    # Output configuration to use (must match a key under outputs)
    output: "salesKafkaOutput"

    # Mapper to transform data before sending (optional)
    mapper: "salesMapper"
